# -*- coding: utf-8 -*-
"""BCI_Processing_and_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Mol4sEa3RruYoDrmb-M8nTJPaX0ELXa1
"""





import matplotlib as plt
import os
import os.path as op
import mne
import numpy as np
import pandas as pd
from pymatreader import read_mat
from glob import glob
from tensorflow.keras.layers import Conv1D, Conv2D, BatchNormalization, LeakyReLU, MaxPool1D, GlobalAveragePooling1D,Dense, Dropout, AveragePooling1D,AveragePooling2D, MaxPool2D
from tensorflow.keras.models import Sequential
from tensorflow import keras
from tensorflow.keras.backend import clear_session
import os
from keras.models import load_model
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

class Preprocessing_and_Classification:
 

  def preProcessData(y)-> np.ndarray:
    y =np.transpose(y)
    y = np.delete(y, (0), axis=0)
    # y =np.array(y["data"])
   # y = np.delete(y, (0), axis=0)
    ch_names = ['FC3', 'FC4', 'C3', 'C4', 'C5', 'C6', 'CP3','CP4']
    ch_types = ['EEG'] * 8
    sampling_freq = 500
    info = mne.create_info(ch_names, sfreq=sampling_freq)
    info.set_montage('standard_1020')
    raw = mne.io.RawArray(y, info)
    mapping = {ch: 'eeg' for ch in ch_names}
    raw.set_channel_types(mapping)
    #raw.resample(512, npad="auto")
    raw.filter(8, 30, fir_design='firwin', picks=['eeg'])
    raw.set_eeg_reference('average', projection=True).apply_proj() 
    raw = raw.interpolate_bads(reset_bads=False)
    ica = mne.preprocessing.ICA(n_components=8, random_state=0)
    ica.fit(raw)
    ica.apply(raw)

    epochs = mne.make_fixed_length_epochs(raw,duration =0.5,overlap=0.1)
   #event_related_plot = epochs.plot_image(picks=['all'])
    epochs= epochs.get_data()   # number of epochs ,channels, length of signal
    #print(epochs.shape)
    epochs = np.swapaxes(epochs,1,2)
    #print(epochs.shape)
    #print(type(epochs))
    return epochs


  def PrepareDataforTraining():

    data_path = 'C:\\Users\\khalilma10\\Downloads\\trainingdata\\'
    all_files_left=os.listdir(data_path+"left")
    all_files_right=os.listdir(data_path+"right")
    all_files_up=os.listdir(data_path+"up")

    left_data_files = [i for i in all_files_left]
    right_data_files = [i for i in all_files_right]
    up_data_files = [i for i in all_files_up]

    #preprocess data
    all_left_data = [Preprocessing_and_Classification.preProcessData(read_mat(data_path+"\\left\\"+i)) for i in left_data_files ]
    all_right_data = [Preprocessing_and_Classification.preProcessData(read_mat(data_path+"\\right\\"+i)) for i in right_data_files ]
    all_up_data = [Preprocessing_and_Classification.preProcessData(read_mat(data_path+"\\up\\"+i)) for i in up_data_files ]
    #create lables
    left_epochs_labels = [len(i)*[0] for i in all_left_data ]
    right_epochs_labels = [len(i)*[1] for i in all_right_data ]
    up_epochs_labels = [len(i)*[2] for i in all_up_data ]
    
    #combine left,right,up
    combined_epochs_data = all_left_data+ all_right_data+all_up_data
    #combine labels
    combined_epochs_labels = left_epochs_labels+right_epochs_labels+up_epochs_labels
    #convert epochs data and labels list to array
    combined_epochs_data_array = np.vstack(combined_epochs_data)
    combined_epochs_labels_array = np.hstack(combined_epochs_labels)
    return combined_epochs_data_array,combined_epochs_labels_array


  def trainModel(combined_epochs_data_array,combined_epochs_labels_array):
    
    model= Preprocessing_and_Classification.cnnmodel()
    model.summary()
    epochs_data_train, epochs_data_test, labels_train, labels_test = train_test_split(combined_epochs_data_array, combined_epochs_labels_array, test_size=0.33)
    print(epochs_data_train.shape, epochs_data_test.shape, labels_train.shape, labels_test.shape)
    scalar =StandardScaler()
    epochs_data_train = scalar.fit_transform(epochs_data_train.reshape(-1,epochs_data_train.shape[-1])).reshape(epochs_data_train.shape)
    history = model.fit(epochs_data_train,labels_train,epochs=20,batch_size=10)
    model.save("my_model.h5", include_optimizer=True)
    
    
  def predictMovement(epochsInput):
   model = keras.models.load_model("my_model.h5")
   movement = model.predict(epochsInput)
   movement=np.argmax(movement,axis=1)
   return movement


  def cnnmodel():

    clear_session()

    model=Sequential()

    model.add(Conv1D (filters=5, kernel_size=3, strides=1, input_shape=(250, 8))) #1  change input shape

    model.add(BatchNormalization())

    model.add(LeakyReLU())

    model.add(MaxPool1D (pool_size=2, strides=2)) #2

    model.add(Conv1D (filters=5, kernel_size=3, strides=1)) #3

    model.add(LeakyReLU())

    model.add(MaxPool1D (pool_size=2, strides=2)) #4

    model.add(Dropout (0.5))

    model.add(Conv1D (filters=5, kernel_size=3, strides=1)) #5
    model.add(LeakyReLU())

    model.add(AveragePooling1D (pool_size=2, strides=2)) #6

    model.add(Dropout (0.5)) 

    model.add(Conv1D (filters=5, kernel_size=3, strides=1)) #7
    model.add(LeakyReLU())

    model.add(AveragePooling1D(pool_size=5, strides=3)) #8 
    model.add(Conv1D (filters=5, kernel_size=3, strides=1)) #9

    model.add(LeakyReLU()) 
    model.add(GlobalAveragePooling1D()) #10

    model.add(Dense(3, activation= 'softmax')) #11
    
    model.compile('adam',loss = 'sparse_categorical_crossentropy',metrics=['accuracy'])
    return model





# combined_epochs_data_array,combined_epochs_labels_array= Preprocessing_and_Classification.PrepareDataforTraining()
# Preprocessing_and_Classification.trainModel(combined_epochs_data_array,combined_epochs_labels_array)






#print(epochs_data_train.shape, epochs_data_test.shape, labels_train.shape, labels_test.shape)






# import matplotlib.pyplot as plt

# plt.plot(history.history['loss'])
# plt.title('model loss')
# plt.ylabel('loss')
# plt.xlabel('epoch')
# plt.legend(['train', 'val'], loc='upper left')
# plt.show()


# plt.plot(history.history['accuracy'])
# plt.title('model accuracy')
# plt.ylabel('accuracy')
# plt.xlabel('epoch')
# plt.legend(['train', 'val'], loc='upper left')
# plt.show()

#import pygds
# d = pygds.GDS()


#d.TriggerEnabled == d.Trigger
#d.Trigger = True
#d.SetConfiguration()

#for c in d.Configs:
#    c.Trigger = True
#d.SetConfiguration()
#d.SamplingRate = 500
#a = d.GetData(d.SamplingRate)
#pred = preProcessData(a)
#model.predict(pred)